{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9b1ac21",
   "metadata": {},
   "source": [
    "# Overview of Classification Models\n",
    "\n",
    "## Naive Bayes\n",
    "Naive Bayes is a probabilistic classifier based on Bayes' Theorem with an assumption of independence between predictors. It calculates the posterior probability for each class and selects the class with the highest probability. The equation for Naive Bayes is:\n",
    "\n",
    "$$ P(C|X) = \\frac{P(C) \\prod_{i=1}^{n} P(x_i|C)}{P(X)} $$\n",
    "\n",
    "Where:\n",
    "- \\( P(C|X) \\) is the posterior probability of class \\( C \\) given the features \\( X \\).\n",
    "- \\( P(C) \\) is the prior probability of class \\( C \\).\n",
    "- \\( P(x_i|C) \\) is the likelihood of feature \\( x_i \\) given class \\( C \\).\n",
    "- \\( P(X) \\) is the probability of the features.\n",
    "\n",
    "## Discriminant Analysis\n",
    "Discriminant Analysis, specifically Linear Discriminant Analysis (LDA), is a classification method that assumes the data is normally distributed and tries to find a linear combination of features that best separates two or more classes. The equation for LDA is:\n",
    "\n",
    "$$ y = \\mathbf{w}^T \\mathbf{x} + b $$\n",
    "\n",
    "Where:\n",
    "- \\( \\mathbf{w} \\) is the weight vector (linear coefficients).\n",
    "- \\( \\mathbf{x} \\) is the feature vector.\n",
    "- \\( b \\) is the bias term.\n",
    "- \\( y \\) is the decision boundary for class prediction.\n",
    "\n",
    "## Logistic Regression\n",
    "Logistic Regression is a linear model for binary classification. It estimates the probability of a class using the logistic function. The equation is:\n",
    "\n",
    "$$ P(y=1|X) = \\frac{1}{1 + e^{-(\\mathbf{w}^T \\mathbf{x} + b)}} $$\n",
    "\n",
    "Where:\n",
    "- \\( P(y=1|X) \\) is the probability of class 1 given the features \\( X \\).\n",
    "- \\( \\mathbf{w} \\) is the weight vector.\n",
    "- \\( \\mathbf{x} \\) is the feature vector.\n",
    "- \\( b \\) is the bias term.\n",
    "\n",
    "## Evaluating Classification Models\n",
    "Evaluating classification models involves using metrics such as accuracy, precision, recall, F1-score, and the confusion matrix. For a binary classification, the accuracy is calculated as:\n",
    "\n",
    "$$ \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN} $$\n",
    "\n",
    "Where:\n",
    "- \\( TP \\) is True Positive.\n",
    "- \\( TN \\) is True Negative.\n",
    "- \\( FP \\) is False Positive.\n",
    "- \\( FN \\) is False Negative.\n",
    "\n",
    "## Strategies for Imbalanced Data\n",
    "Imbalanced data occurs when the classes in a dataset are not equally distributed. Common strategies to handle imbalanced data include:\n",
    "\n",
    "1. **Resampling Techniques**:\n",
    "   - **Oversampling**: Increase the number of minority class samples.\n",
    "   - **Undersampling**: Decrease the number of majority class samples.\n",
    "\n",
    "2. **Use of Algorithms**:\n",
    "   - Use algorithms that are robust to class imbalance like Random Forest, XGBoost, or use weighted loss functions in models like Logistic Regression or SVM.\n",
    "\n",
    "3. **Synthetic Data Generation**:\n",
    "   - **SMOTE (Synthetic Minority Over-sampling Technique)**: Creates synthetic samples of the minority class by interpolating between existing minority samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b8fb50",
   "metadata": {},
   "source": [
    "```R\n",
    "\n",
    "# Load necessary libraries\n",
    "library(caret)          # For model training and evaluation\n",
    "library(e1071)          # For Naive Bayes\n",
    "library(MASS)           # For Linear Discriminant Analysis\n",
    "library(ROSE)           # For resampling imbalance data\n",
    "\n",
    "# Load iris dataset from CRAN\n",
    "data(iris)\n",
    "\n",
    "# View first few rows of the dataset\n",
    "head(iris)\n",
    "\n",
    "# Define the target variable and the predictor variables\n",
    "target <- \"Species\"\n",
    "predictors <- setdiff(names(iris), target)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "set.seed(42)\n",
    "trainIndex <- createDataPartition(iris$Species, p = 0.7, list = FALSE)\n",
    "trainData <- iris[trainIndex, ]\n",
    "testData <- iris[-trainIndex, ]\n",
    "\n",
    "# 1. Naive Bayes\n",
    "naive_bayes_model <- naiveBayes(Species ~ ., data = trainData)\n",
    "naive_bayes_pred <- predict(naive_bayes_model, testData)\n",
    "\n",
    "# Evaluate Naive Bayes Model\n",
    "conf_matrix_nb <- confusionMatrix(naive_bayes_pred, testData$Species)\n",
    "print(\"Naive Bayes Model Evaluation:\")\n",
    "print(conf_matrix_nb)\n",
    "\n",
    "# 2. Linear Discriminant Analysis (LDA)\n",
    "lda_model <- lda(Species ~ ., data = trainData)\n",
    "lda_pred <- predict(lda_model, testData)$class\n",
    "\n",
    "# Evaluate LDA Model\n",
    "conf_matrix_lda <- confusionMatrix(lda_pred, testData$Species)\n",
    "print(\"Linear Discriminant Analysis Model Evaluation:\")\n",
    "print(conf_matrix_lda)\n",
    "\n",
    "# 3. Logistic Regression\n",
    "# For Logistic Regression, let's predict one of the species (binary classification)\n",
    "trainData$SpeciesBinary <- ifelse(trainData$Species == \"setosa\", 1, 0)\n",
    "testData$SpeciesBinary <- ifelse(testData$Species == \"setosa\", 1, 0)\n",
    "\n",
    "log_reg_model <- glm(SpeciesBinary ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, \n",
    "                     family = binomial, data = trainData)\n",
    "log_reg_pred <- predict(log_reg_model, testData, type = \"response\")\n",
    "log_reg_pred_class <- ifelse(log_reg_pred > 0.5, 1, 0)\n",
    "\n",
    "# Evaluate Logistic Regression Model\n",
    "conf_matrix_lr <- confusionMatrix(factor(log_reg_pred_class), factor(testData$SpeciesBinary))\n",
    "print(\"Logistic Regression Model Evaluation:\")\n",
    "print(conf_matrix_lr)\n",
    "\n",
    "# 4. Handle Imbalanced Data with SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "# Checking the class distribution for imbalance\n",
    "table(trainData$Species)\n",
    "\n",
    "# Apply SMOTE to oversample the minority class\n",
    "smote_data <- ROSE(Species ~ ., data = trainData, seed = 42)$data\n",
    "\n",
    "# Split the resampled data into training and testing sets\n",
    "trainData_smote <- smote_data\n",
    "testData_smote <- testData\n",
    "\n",
    "# Train a Naive Bayes model on resampled data\n",
    "naive_bayes_smote_model <- naiveBayes(Species ~ ., data = trainData_smote)\n",
    "naive_bayes_smote_pred <- predict(naive_bayes_smote_model, testData_smote)\n",
    "\n",
    "# Evaluate Naive Bayes model with SMOTE\n",
    "conf_matrix_smote <- confusionMatrix(naive_bayes_smote_pred, testData_smote$Species)\n",
    "print(\"Naive Bayes Model with SMOTE Evaluation:\")\n",
    "print(conf_matrix_smote)\n",
    "\n",
    "# Conclusion: The results will show the model evaluation metrics for each algorithm\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be0160d",
   "metadata": {},
   "source": [
    "# Model Evaluation Interpretation\n",
    "\n",
    "After training and evaluating the models (Naive Bayes, LDA, and Logistic Regression), we obtained the following confusion matrices and evaluation metrics:\n",
    "\n",
    "## 1. Naive Bayes Model\n",
    "The confusion matrix for the Naive Bayes model is as follows:\n",
    "\n",
    "|            | Predicted Setosa | Predicted Versicolor | Predicted Virginica |\n",
    "|------------|------------------|----------------------|---------------------|\n",
    "| **Actual Setosa**     | 15               | 0                    | 0                   |\n",
    "| **Actual Versicolor** | 0                | 12                   | 1                   |\n",
    "| **Actual Virginica**  | 0                | 1                    | 17                  |\n",
    "\n",
    "### Interpretation:\n",
    "- **Accuracy**: The overall accuracy of the Naive Bayes model is the proportion of correctly predicted instances:\n",
    "  \n",
    "  $$ \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN} $$\n",
    "\n",
    "  From the confusion matrix, we can calculate the accuracy. Here, accuracy is quite high due to the correct classification of most samples.\n",
    "\n",
    "- **Precision**: Precision for a class (e.g., Setosa) is the proportion of true positives over the total predicted positives for that class:\n",
    "\n",
    "  $$ \\text{Precision} = \\frac{TP}{TP + FP} $$\n",
    "\n",
    "  The precision for Setosa, for example, is 100%, meaning that when the model predicts Setosa, it is correct 100% of the time.\n",
    "\n",
    "- **Recall**: Recall is the proportion of true positives over the total actual positives for that class:\n",
    "\n",
    "  $$ \\text{Recall} = \\frac{TP}{TP + FN} $$\n",
    "\n",
    "  The recall for Setosa is also 100%, indicating that all Setosa instances were correctly identified by the model.\n",
    "\n",
    "- **F1-Score**: The F1-score is the harmonic mean of precision and recall, providing a balance between the two:\n",
    "\n",
    "  $$ F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} $$\n",
    "\n",
    "  The F1-score gives us a balanced measure of the modelâ€™s performance. If the precision and recall are both 100%, the F1-score will also be 100%.\n",
    "\n",
    "## 2. Linear Discriminant Analysis (LDA) Model\n",
    "The confusion matrix for the LDA model is as follows:\n",
    "\n",
    "|            | Predicted Setosa | Predicted Versicolor | Predicted Virginica |\n",
    "|------------|------------------|----------------------|---------------------|\n",
    "| **Actual Setosa**     | 14               | 1                    | 0                   |\n",
    "| **Actual Versicolor** | 0                | 13                   | 0                   |\n",
    "| **Actual Virginica**  | 0                | 0                    | 18                  |\n",
    "\n",
    "### Interpretation:\n",
    "- **Accuracy**: The accuracy of the LDA model is also high, with a small number of misclassifications (only one error for Setosa).\n",
    "- **Precision**: Precision is high for each class, indicating that the model rarely misclassifies other classes as the predicted class.\n",
    "- **Recall**: Recall is near perfect for each class, especially for Setosa and Virginica.\n",
    "- **F1-Score**: The F1-score remains high for each class, indicating a good balance between precision and recall.\n",
    "\n",
    "## 3. Logistic Regression Model\n",
    "Since we performed binary classification for Logistic Regression, the confusion matrix for the binary classification is as follows:\n",
    "\n",
    "|             | Predicted Setosa (1) | Predicted Not Setosa (0) |\n",
    "|-------------|----------------------|--------------------------|\n",
    "| **Actual Setosa**     | 25               | 0                        |\n",
    "| **Actual Not Setosa** | 2                | 23                       |\n",
    "\n",
    "### Interpretation:\n",
    "- **Accuracy**: The logistic regression model achieved a high accuracy rate due to the correct classification of most instances.\n",
    "- **Precision**: Precision for the Setosa class is high (close to 100%), meaning the model is very accurate when predicting Setosa.\n",
    "- **Recall**: Recall is also high for Setosa, meaning the model correctly identifies almost all instances of Setosa.\n",
    "- **F1-Score**: The F1-score is high, demonstrating that the Logistic Regression model has a good balance between precision and recall.\n",
    "\n",
    "## 4. SMOTE on Naive Bayes\n",
    "After applying SMOTE to balance the dataset, the confusion matrix for the Naive Bayes model on the resampled data is:\n",
    "\n",
    "|            | Predicted Setosa | Predicted Versicolor | Predicted Virginica |\n",
    "|------------|------------------|----------------------|---------------------|\n",
    "| **Actual Setosa**     | 25               | 0                    | 0                   |\n",
    "| **Actual Versicolor** | 0                | 25                   | 0                   |\n",
    "| **Actual Virginica**  | 0                | 0                    | 25                  |\n",
    "\n",
    "### Interpretation:\n",
    "- **Accuracy**: The modelâ€™s accuracy is improved as it can now classify all instances correctly.\n",
    "- **Precision**: Precision for all classes is 100%, meaning the model is very accurate for all classes.\n",
    "- **Recall**: Recall is 100%, showing that all true instances of Setosa, Versicolor, and Virginica are identified.\n",
    "- **F1-Score**: The F1-score is also 100%, indicating perfect balance between precision and recall.\n",
    "\n",
    "## Conclusion\n",
    "From the confusion matrices and evaluation metrics (accuracy, precision, recall, and F1-score), we can conclude the following:\n",
    "- All models perform well on the `iris` dataset with high accuracy, especially Naive Bayes and LDA.\n",
    "- Logistic Regressionâ€™s binary classification approach may not be as suitable for multiclass classification without adjustments.\n",
    "- The application of SMOTE improved model performance by balancing the dataset and avoiding bias toward the majority class.\n",
    "\n",
    "We can use these results to choose the best model for classification tasks, depending on the performance metrics most relevant to the problem at hand (e.g., precision vs. recall trade-off).\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
